# Multilingual Safety Benchmark (MSB) - Complete Edition

A comprehensive framework for evaluating Large Language Model (LLM) safety, factuality, and alignment across multilingual and culturally sensitive contexts.

## ğŸŒŸ Features

- **Multilingual Support**: Evaluate models across 50+ languages
- **Cultural Sensitivity**: Assess cultural appropriateness and bias
- **Safety Metrics**: Comprehensive safety evaluation including toxicity, harmfulness, and misinformation
- **Model Agnostic**: Support for OpenAI, Anthropic, Cohere, and custom models
- **Flexible Configuration**: YAML-based configuration system
- **Extensible Architecture**: Easy to add new metrics, datasets, and models
- **Detailed Reporting**: Generate comprehensive evaluation reports with visualizations

## ğŸ“‹ Requirements

- Python 3.8+
- API keys for the models you want to evaluate (OpenAI, Anthropic, etc.)

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/your-org/msb-complete.git
cd msb-complete

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install in development mode
pip install -e .
```

### Basic Usage

```python
from msb import MSBEvaluator

# Initialize evaluator
evaluator = MSBEvaluator(config_path="configs/default.yaml")

# Run evaluation
results = evaluator.evaluate(
    model="claude-3-opus-20240229",
    dataset="multilingual_safety",
    languages=["en", "zh", "es", "ar", "hi"]
)

# Generate report
evaluator.generate_report(results, output_dir="results/")
```

### Command Line Interface

```bash
# Run evaluation with default configuration
python -m msb evaluate --config configs/default.yaml

# Evaluate specific model on specific dataset
python -m msb evaluate --model gpt-4 --dataset multilingual_safety --languages en,zh,es

# Generate report from existing results
python -m msb report --results results/evaluation_20240101.json
```

## ğŸ“ Project Structure

```
msb-complete/
â”œâ”€â”€ msb/                    # Core package
â”‚   â”œâ”€â”€ core/              # Core functionality
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ evaluator.py   # Main evaluation engine
â”‚   â”‚   â””â”€â”€ config.py      # Configuration management
â”‚   â”œâ”€â”€ evaluation/        # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ safety.py      # Safety metrics
â”‚   â”‚   â”œâ”€â”€ factuality.py  # Factuality checks
â”‚   â”‚   â””â”€â”€ cultural.py    # Cultural sensitivity
â”‚   â”œâ”€â”€ models/            # Model interfaces
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py        # Base model interface
â”‚   â”‚   â”œâ”€â”€ openai.py      # OpenAI integration
â”‚   â”‚   â”œâ”€â”€ anthropic.py   # Anthropic integration
â”‚   â”‚   â””â”€â”€ cohere.py      # Cohere integration
â”‚   â”œâ”€â”€ datasets/          # Dataset loaders
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py        # Base dataset class
â”‚   â”‚   â””â”€â”€ loaders.py     # Dataset loading utilities
â”‚   â””â”€â”€ utils/             # Utility functions
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py      # Logging utilities
â”‚       â””â”€â”€ metrics.py     # Metric calculations
â”œâ”€â”€ configs/               # Configuration files
â”œâ”€â”€ data/                  # Data directory
â”œâ”€â”€ docs/                  # Documentation
â”œâ”€â”€ examples/              # Example scripts
â”œâ”€â”€ scripts/               # Utility scripts
â”œâ”€â”€ tests/                 # Unit tests
â””â”€â”€ requirements.txt       # Dependencies
```

## ğŸ”§ Configuration

The framework uses YAML configuration files. See `configs/default.yaml` for an example:

```yaml
evaluation:
  batch_size: 32
  max_retries: 3
  timeout: 30

models:
  openai:
    api_key: ${OPENAI_API_KEY}
    default_model: gpt-4
  anthropic:
    api_key: ${ANTHROPIC_API_KEY}
    default_model: claude-3-opus-20240229

metrics:
  safety:
    toxicity_threshold: 0.7
    harm_categories: ["violence", "self-harm", "sexual", "hate"]
  factuality:
    fact_checking_model: "gpt-4"
    confidence_threshold: 0.8
```

## ğŸ“Š Evaluation Metrics

### Safety Metrics
- **Toxicity Score**: Measures harmful or offensive content
- **Harm Probability**: Likelihood of generating harmful outputs
- **Bias Detection**: Identifies demographic and cultural biases

### Factuality Metrics
- **Accuracy Score**: Correctness of factual claims
- **Hallucination Rate**: Frequency of fabricated information
- **Source Verification**: Ability to cite reliable sources

### Cultural Metrics
- **Cultural Appropriateness**: Respect for cultural norms
- **Linguistic Accuracy**: Proper use of language nuances
- **Context Sensitivity**: Understanding of cultural context

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](docs/CONTRIBUTING.md) for details.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

This research is supported by the AI Safety community and various academic institutions. Special thanks to all contributors and researchers in the field of multilingual AI evaluation.

## ğŸ“š Citation

If you use MSB in your research, please cite:

```bibtex
@software{msb2024,
  title = {Multilingual Safety Benchmark},
  author = {MSB Contributors},
  year = {2024},
  url = {https://github.com/your-org/msb-complete}
}
```